{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Learner algorithm (PRISM): Notes\n",
    "\n",
    "Use lecture slides, book chapters or the original paper to understand the PRISM algorithm. \n",
    "\n",
    "Read this notebook. Then, __IN A SEPARATE NOTEBOOK__, implement the algorithm. Implement it step-by-step, add explanations of each step, keep the code clean and modular. Finally, test the correctness of your algorithm on the contact lenses [dataset](contact_lenses.csv) discussed in class, to see if it produces the same rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating some classes\n",
    "\n",
    "These are the classes you made for the lab, thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    def __init__(self, class_label):\n",
    "        self.conditions = []  # list of conditions\n",
    "        self.class_label = class_label  # rule class\n",
    "        self.accuracy = 0\n",
    "        self.coverage = 0\n",
    "\n",
    "    def addCondition(self, condition):\n",
    "        self.conditions.append(condition)\n",
    "\n",
    "    def setParams(self, accuracy, coverage):\n",
    "        self.accuracy = accuracy\n",
    "        self.coverage = coverage\n",
    "    \n",
    "    # Human-readable printing of this Rule\n",
    "    def __repr__(self):\n",
    "        return \"If {} then {}. Coverage:{}, accuracy: {}\".format(self.conditions, self.class_label,\n",
    "                                                                 self.coverage, self.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of conditions contains several objects of class _Condition_. \n",
    "\n",
    "Each condition includes the _attribute name_ and the _value_. \n",
    "\n",
    "If the _value_ is numeric, then the condition also includes an additional field `true_false` which means the following: \n",
    "- *if true_false == True then values are >= value* \n",
    "- *if true_false == False then values are < value*\n",
    "- If *true_false is None*, then this condition is simply of form *categorical attribute = value*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Condition:\n",
    "    def __init__(self, attribute, value, true_false = None):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.true_false = true_false\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.true_false is None:\n",
    "            return \"{}={}\".format(self.attribute, self.value)\n",
    "        else:\n",
    "            return \"{}>={}:{}\".format(self.attribute, self.value, self.true_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Algorithm\n",
    "\n",
    "I'll be using the approach you laid out in the Algorithm Notes section. There are some differences, notably in the stucture. I noticed that refining rules and finding the first rule are very similar, so I made them one function (refine_rule()). \n",
    "\n",
    "There is maybe one big decision I made that could differ from the normal algorithm. I added a condition to break refining if we ever fail to find an additional rule that improves our accuracy. So if a rule starts out at 70% accruate, and any other possible rule we could add it to keeps the overall rules accuracy at 70%, we do not include it. This is not quite what the algorithm does, but it will make the examples work better. The main motivation for my adding it, is if we do not include this and min_coverage is 1 (like it is in the eye example), then the PRISM algorithm will often give complicated rules singling out a single person.  \n",
    "\n",
    "For the larger datasets in the next section, I will repeat all calculations with both an algorithm including the refinement stopper and one without. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_rule(columns, covered_subset, class_label, min_coverage=30, min_accuracy=0.6, current_rule = None, colTypes = None):\n",
    "    \n",
    "    # If we're refining from None, create an empty Rule\n",
    "    if (current_rule == None):\n",
    "        current_rule = Rule(class_label)\n",
    "        \n",
    "\n",
    "    # Some helpful variables for tracking \n",
    "    bestAcc = 0\n",
    "    bestCov = 0\n",
    "    bestCondition = None\n",
    "    curAcc = 0\n",
    "    curCov = 0 \n",
    "\n",
    "    # We start by testing all possible columns/traits until we find the best one:\n",
    "    for trait in columns:\n",
    "        if trait == columns[-1]:\n",
    "            continue\n",
    "    \n",
    "        # If the trait is numeric\n",
    "        if colTypes[trait].loc['type'] == \"numeric\":\n",
    "            # We'll split by less than and ge. \n",
    "            \n",
    "            # Less than:\n",
    "            \n",
    "            for level in covered_subset[trait].unique().tolist():\n",
    "                ruleSubset = covered_subset[covered_subset[trait] < level] # Split by less than\n",
    "                \n",
    "                # Make sure we don't divide by 0!\n",
    "                if (len(ruleSubset) > 0):\n",
    "                    curAcc = len(ruleSubset[ruleSubset[columns[-1]] == class_label]) / len(ruleSubset)\n",
    "                    curCov = len(ruleSubset[ruleSubset[columns[-1]] == class_label])\n",
    "                    # Evaluate effectiveness of the rule\n",
    "                    if (curCov >= min_coverage):\n",
    "                        if (curAcc > bestAcc):\n",
    "                            bestAcc = curAcc\n",
    "                            bestCov = curCov\n",
    "                            bestCondition = Condition(trait, level, False)\n",
    "                        if (curAcc == bestAcc):\n",
    "                            if (curCov > bestCov):\n",
    "                                bestCov = curCov\n",
    "                                bestCondition = Condition(trait, level, False)\n",
    "                \n",
    "                # Now check the >= case:\n",
    "                ruleSubset = covered_subset[covered_subset[trait] >= level] # Split by ge\n",
    "                if (len(ruleSubset) > 0):\n",
    "                    curAcc = len(ruleSubset[ruleSubset[columns[-1]] == class_label]) / len(ruleSubset)\n",
    "                    curCov = len(ruleSubset[ruleSubset[columns[-1]] == class_label])\n",
    "                    # Evaluate goodness of the rule\n",
    "                    if (curCov >= min_coverage):\n",
    "                        if (curAcc > bestAcc):\n",
    "                            bestAcc = curAcc\n",
    "                            bestCov = curCov\n",
    "                            bestCondition = Condition(trait, level, True)\n",
    "                        if (curAcc == bestAcc):\n",
    "                            if (curCov > bestCov):\n",
    "                                bestCov = curCov\n",
    "                                bestCondition = Condition(trait, level, True)\n",
    "                            \n",
    "            continue # Go to the next loop\n",
    "        \n",
    "         \n",
    "        # If our trait is categorical:\n",
    "        for typeTrait in covered_subset[trait].unique().tolist():\n",
    "            ruleSubset = covered_subset[covered_subset[trait] == typeTrait]\n",
    "            \n",
    "            # Now test for accuracy and coverage of the rule:\n",
    "            curAcc = len(ruleSubset[ruleSubset[columns[-1]] == class_label]) / len(ruleSubset)\n",
    "            curCov = len(ruleSubset[ruleSubset[columns[-1]] == class_label])\n",
    "            if (curCov >= min_coverage):\n",
    "                if (curAcc > bestAcc):\n",
    "                    bestAcc = curAcc\n",
    "                    bestCov = curCov\n",
    "                    bestCondition = Condition(trait, typeTrait)\n",
    "\n",
    "                # If we have another rule with the same accuracy, \n",
    "                # choose the one with the better coverage\n",
    "                if (curAcc == bestAcc):\n",
    "                    if curCov > bestCov:\n",
    "                        bestCov = curCov\n",
    "                        bestCondition = Condition(trait, typeTrait)\n",
    "\n",
    "    # If refining does not improve our accuracy, we break\n",
    "    if (bestAcc == current_rule.accuracy):\n",
    "        return(None, None)\n",
    "    \n",
    "    # If we have no rule, break\n",
    "    if (bestCondition == None):\n",
    "        return(None, None)\n",
    "    \n",
    "    # Otherwise, add the rule to the list and\n",
    "    # shorten the subset\n",
    "    current_rule.addCondition(bestCondition)\n",
    "    current_rule.setParams(bestAcc, bestCov)\n",
    "    \n",
    "    # Numeric makes things a little stranger\n",
    "    if colTypes[bestCondition.attribute].loc['type'] == \"numeric\":\n",
    "        if bestCondition.true_false:\n",
    "            covered_subset = covered_subset[covered_subset[bestCondition.attribute] >= bestCondition.value]\n",
    "        else:\n",
    "            covered_subset = covered_subset[covered_subset[bestCondition.attribute] < bestCondition.value]\n",
    "    else:\n",
    "        covered_subset = covered_subset[covered_subset[bestCondition.attribute] == bestCondition.value]\n",
    "    return (current_rule, covered_subset)\n",
    "                            \n",
    "    \n",
    "    \n",
    "\n",
    "# Ok, now we can learn_one_rule!\n",
    "\n",
    "def learn_one_rule(columns, data, class_label, min_coverage=30, min_accuracy=0.6, colType = None):\n",
    "    # Make a copy of columns so we can delete from it. \n",
    "    # Otherwise we'd be destorying columns\n",
    "    columnsCopy = columns.copy().tolist()\n",
    "    \n",
    "    # Refine once from None, this will create the best first rule we want\n",
    "    current_rule, covered_subset = refine_rule(columnsCopy, data.copy(), class_label, min_coverage, min_accuracy, None, colType)\n",
    "    \n",
    "    # If we got None out, it means there is no rule that meets min_coverage\n",
    "    if (current_rule == None):\n",
    "        return (None, None)\n",
    "    \n",
    "    # Remove the trait from the list of eligbile traits\n",
    "    columnsCopy.remove(current_rule.conditions[-1].attribute)\n",
    "    \n",
    "    # We will refine while our rule isn't perfect and there are \n",
    "    # traits left to add\n",
    "    while current_rule.accuracy < 1 and len(columnsCopy) > 1:\n",
    "        temp_rule, temp_subset = refine_rule(columnsCopy, covered_subset,\n",
    "                                             class_label, min_coverage, min_accuracy, current_rule, colType)\n",
    "        # Just remove the trait we just used\n",
    "        if (temp_rule != None):\n",
    "            current_rule = temp_rule\n",
    "            covered_subset = temp_subset\n",
    "            columnsCopy.remove(current_rule.conditions[-1].attribute)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    if current_rule.accuracy < min_accuracy:\n",
    "        return (None, None)\n",
    "    \n",
    "    return (current_rule, covered_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the main algorithm `learn_rules` takes as parameters list of columns, with the last column representing the class label, and the original data in form of pandas dataframe. Optionally, you should be able to specify the list of classes that you are interested to explore first. Two optional threshold parameters `min_coverage` and `min_accuracy` set up the conditions of rule's validity for a given dataset.\n",
    "\n",
    "We had to implement the step where we make the data domain smaller, I just dropped by matching up the ID's of the two datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "    # We can pre-process to determine if something is a numeric trait. \n",
    "    # We'll use two rules here:\n",
    "    # 1. If the Trait is all numeric\n",
    "    # 2. If there are a certain number of unique oberservations\n",
    "    #    in the dataset, say more than 10. \n",
    "    # \n",
    "    # Returns a Pandas Dataframe with the columns of `columns`\n",
    "    # and a row of \"type\" with the proper type of the variable\n",
    "def learn_categories(columns, data, min_threshold = 10):\n",
    "    types = []\n",
    "    cols = data.columns\n",
    "    for column in cols:\n",
    "        if data[column].dtype == 'int64' and len(data[column].unique()) >= min_threshold::\n",
    "            types.append(\"numeric\")\n",
    "        else:\n",
    "            types.append(\"categorical\")\n",
    "\n",
    "    colType = pd.DataFrame(columns = cols)\n",
    "\n",
    "    colType.loc['type'] = types\n",
    "    return colType\n",
    "    \n",
    "    \n",
    "def learn_rules (columns, data, classes=None, \n",
    "    min_coverage = 30, min_accuracy = 0.6):\n",
    "    # List of final rules\n",
    "    rules = []\n",
    "    \n",
    "    # If list of classes of interest is not provided - it is extracted from the last column of data\n",
    "    if classes is not None:\n",
    "        class_labels = classes\n",
    "    else:\n",
    "        class_labels = data[columns[-1]].unique().tolist()\n",
    "\n",
    "    current_data = data.copy()\n",
    "\n",
    "    # Get the Column types (categorical vs. numeric)\n",
    "    colType = learn_categories(columns, data)\n",
    "\n",
    "    \n",
    "    \n",
    "    # This follows the logic of the original PRISM algorithm\n",
    "    # It processes each class in turn. \n",
    "    for class_label in class_labels:\n",
    "        done = False\n",
    "        while len(current_data) > min_coverage and not done:\n",
    "            # Learn one rule \n",
    "            rule, subset = learn_one_rule(columns, current_data, class_label, min_coverage, min_accuracy, colType)\n",
    "            \n",
    "            # If the best rule does not pass the coverage threshold - we are done with this class\n",
    "            if rule is None:\n",
    "                break\n",
    "\n",
    "            # If we get the rule with accuracy and coverage above threshold\n",
    "            \n",
    "            if rule.accuracy >= min_accuracy:\n",
    "                rules.append(rule)\n",
    "\n",
    "                # We're going to say that `subset` is the rows that need to be removed. \n",
    "                # We can drop by matching up the indices\n",
    "                current_data = current_data.drop(index=subset.index)\n",
    "                   \n",
    "            else:\n",
    "                done = True         \n",
    "                \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correctness test  \n",
    "\n",
    "Made no changes to the code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your algorithm on the original dataset from the PRISM paper.\n",
    "\n",
    "The dataset was downloaded from [here](https://archive.ics.uci.edu/ml/datasets/Lenses). The CSV version is included in this repository.\n",
    "\n",
    "**Attribute Information**:\n",
    "\n",
    "3 Classes:\n",
    "- __1__ : the patient should be fitted with __hard__ contact lenses,\n",
    "- __2__ : the patient should be fitted with __soft__ contact lenses,\n",
    "- __3__ : the patient should __not__ be fitted with contact lenses.\n",
    "\n",
    "\n",
    "Attributes:\n",
    "1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic\n",
    "2. spectacle prescription:  (1) myope, (2) hypermetrope\n",
    "3. astigmatic:     (1) no, (2) yes\n",
    "4. tear production rate:  (1) reduced, (2) normal\n",
    "\n",
    "Presbyopia is physiological insufficiency of accommodation associated with the aging of the eye that results in progressively worsening ability to focus clearly on close objects. So \"age=presbiopic\" means old.\n",
    "\n",
    "Hypermetropia: far-sightedness, also known as long-sightedness - cannot see close.\n",
    "Myopia: nearsightedness - cannot see at distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'spectacles', 'astigmatism', 'tear production rate',\n",
       "       'lenses type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_file = \"contact_lenses.csv\"\n",
    "data = pd.read_csv(data_file, index_col=['id'])\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace numbers with actual values - for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "conditions = [ data['lenses type'].eq(1), data['lenses type'].eq(2), data['lenses type'].eq(3)]\n",
    "choices = [\"hard\",\"soft\",\"none\"]\n",
    "\n",
    "data['lenses type'] = np.select(conditions, choices)\n",
    "\n",
    "# age groups\n",
    "conditions = [ data['age'].eq(1), data['age'].eq(2), data['age'].eq(3)]\n",
    "choices = [\"young\",\"medium\",\"old\"]\n",
    "\n",
    "data['age'] = np.select(conditions, choices)\n",
    "\n",
    "# spectacles\n",
    "conditions = [ data['spectacles'].eq(1), data['spectacles'].eq(2)]\n",
    "choices = [\"nearsighted\",\"farsighted\"]\n",
    "\n",
    "data['spectacles'] = np.select(conditions, choices)\n",
    "\n",
    "# astigmatism\n",
    "conditions = [ data['astigmatism'].eq(1), data['astigmatism'].eq(2)]\n",
    "choices = [\"no\",\"yes\"]\n",
    "\n",
    "data['astigmatism'] = np.select(conditions, choices)\n",
    "\n",
    "# tear production rate\n",
    "conditions = [ data['tear production rate'].eq(1), data['tear production rate'].eq(2)]\n",
    "choices = [\"reduced\",\"normal\"]\n",
    "\n",
    "data['tear production rate'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test (do not run it before you finished the implementation of the rule learning algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "spectacles\n",
      "astigmatism\n",
      "tear production rate\n",
      "lenses type\n",
      "If [tear production rate=reduced] then none. Coverage:12, accuracy: 1.0\n",
      "If [astigmatism=no, spectacles=farsighted] then soft. Coverage:3, accuracy: 1.0\n",
      "If [astigmatism=no, age=young] then soft. Coverage:1, accuracy: 1.0\n",
      "If [astigmatism=no, age=medium] then soft. Coverage:1, accuracy: 1.0\n",
      "If [age=young] then hard. Coverage:2, accuracy: 1.0\n",
      "If [spectacles=nearsighted, astigmatism=yes] then hard. Coverage:2, accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "column_list = data.columns.to_numpy()\n",
    "rules = learn_rules (column_list, data, None, 1, 0.95)\n",
    "for rule in rules[:20]:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Results\n",
    "\n",
    "If [tear production rate=reduced] then none. Coverage:12, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, spectacles=farsighted] then soft. Coverage:3, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, age=young] then soft. Coverage:1, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, age=medium] then soft. Coverage:1, accuracy: 1.0\n",
    "\n",
    "If [age=young] then hard. Coverage:2, accuracy: 1.0\n",
    "\n",
    "If [spectacles=nearsighted, astigmatism=yes] then hard. Coverage:2, accuracy: 1.0\n",
    "\n",
    "They match!!\n",
    "\n",
    "## Your Results\n",
    "My results are given below for comparison:\n",
    "\n",
    "If [tear production rate=reduced] then none. Coverage:12, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, spectacles=farsighted] then soft. Coverage:3, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, age=young] then soft. Coverage:1, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, age=medium] then soft. Coverage:1, accuracy: 1.0\n",
    "\n",
    "If [age=young] then hard. Coverage:2, accuracy: 1.0\n",
    "\n",
    "If [spectacles=nearsighted, astigmatism=yes] then hard. Coverage:2, accuracy: 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating the Above Step removing the Refining Stop Condition\n",
    "\n",
    "As I mentioned, the refining stop condition makes things trickier for the glasses example specifically, so I'll illustrate what I found here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_rule_rf(columns, covered_subset, class_label, min_coverage=30, min_accuracy=0.6, current_rule = None):\n",
    "    \n",
    "    if (current_rule == None):\n",
    "        current_rule = Rule(class_label)\n",
    "        \n",
    "\n",
    "    bestAcc = 0\n",
    "    bestCov = 0\n",
    "    bestCondition = None\n",
    "    curAcc = 0\n",
    "    curCov = 0 \n",
    "\n",
    "    # We start by testing all possible columns/traits until we find the best one:\n",
    "    for trait in columns:\n",
    "        if trait == columns[-1]:\n",
    "            continue\n",
    "        for typeTrait in covered_subset[trait].unique().tolist():\n",
    "            ruleSubset = covered_subset[covered_subset[trait] == typeTrait]\n",
    "            # Now test for accuracy of the rule:\n",
    "            # We can use .size which normally \n",
    "            # returns total number of element in the dataframe\n",
    "            # but since we would divide both numbers by the number of columns\n",
    "            # to get accuracy, we can just do this\n",
    "\n",
    "            curAcc = len(ruleSubset[ruleSubset[columns[-1]] == class_label]) / len(ruleSubset)\n",
    "            curCov = len(ruleSubset[ruleSubset[columns[-1]] == class_label])\n",
    "            if (curCov >= min_coverage):\n",
    "                if (curAcc > bestAcc):\n",
    "                    bestAcc = curAcc\n",
    "                    bestCov = curCov\n",
    "                    bestCondition = Condition(trait, typeTrait)\n",
    "\n",
    "                # If we have another rule with the same accuracy, \n",
    "                # choose the one with the better coverage\n",
    "                if (curAcc == bestAcc):\n",
    "                    if curCov > bestCov:\n",
    "                        bestCov = curCov\n",
    "                        bestCondition = Condition(trait, typeTrait)\n",
    "\n",
    "\n",
    "    # If we have no rule, break\n",
    "    if (bestCondition == None):\n",
    "        return(None, None)\n",
    "    # Otherwise, add the rule to the list and\n",
    "    # shorten the subset\n",
    "    current_rule.addCondition(bestCondition)\n",
    "    current_rule.setParams(bestAcc, bestCov)\n",
    "    covered_subset = covered_subset[covered_subset[bestCondition.attribute] == bestCondition.value]\n",
    "    return (current_rule, covered_subset)\n",
    "                            \n",
    "    \n",
    "    \n",
    "\n",
    "# Ok, now we can learn_one_rule!\n",
    "\n",
    "def learn_one_rule_rf(columns, data, class_label, min_coverage=30, min_accuracy=0.6):\n",
    "    # Make a copy of columns so we can delete from it. \n",
    "    # Otherwise we'd be destorying columns\n",
    "    columnsCopy = columns.copy()\n",
    "    \n",
    "    # Refine once, this will create the rule we want\n",
    "    current_rule, covered_subset = refine_rule_rf(columnsCopy, data.copy(), class_label, min_coverage, min_accuracy, None)\n",
    "    \n",
    "    # If we got None out, it means there is no rule that meets min_coverage\n",
    "    if (current_rule == None):\n",
    "        return (None, None)\n",
    "    \n",
    "    # Remove the trait from the list of eligbile traits\n",
    "    columnsCopy.remove(current_rule.conditions[-1].attribute)\n",
    "    \n",
    "    # We will refine while our rule isn't perfect and there are \n",
    "    # traits left to add\n",
    "    while current_rule.accuracy < 1 and len(columnsCopy) > 1:\n",
    "        temp_rule, temp_subset = refine_rule_rf(columnsCopy, covered_subset,\n",
    "                                             class_label, min_coverage, min_accuracy, current_rule)\n",
    "        if (temp_rule != None):\n",
    "            current_rule = temp_rule\n",
    "            covered_subset = temp_subset\n",
    "            columnsCopy.remove(current_rule.conditions[-1].attribute)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    if current_rule.accuracy < min_accuracy:\n",
    "        return (None, None)\n",
    "    \n",
    "    return (current_rule, covered_subset)\n",
    "\n",
    "def learn_rules_rf (columns, data, classes=None, \n",
    "                 min_coverage = 30, min_accuracy = 0.6):\n",
    "    # List of final rules\n",
    "    rules = []\n",
    "    \n",
    "    # If list of classes of interest is not provided - it is extracted from the last column of data\n",
    "    if classes is not None:\n",
    "        class_labels = classes\n",
    "    else:\n",
    "        class_labels = data[columns[-1]].unique().tolist()\n",
    "\n",
    "    current_data = data.copy()\n",
    "    \n",
    "    # This follows the logic of the original PRISM algorithm\n",
    "    # It processes each class in turn. \n",
    "    for class_label in class_labels:\n",
    "        done = False\n",
    "        while len(current_data) > min_coverage and not done:\n",
    "            # Learn one rule \n",
    "            rule, subset = learn_one_rule_rf(columns, current_data, class_label, min_coverage, min_accuracy)\n",
    "            \n",
    "            # If the best rule does not pass the coverage threshold - we are done with this class\n",
    "            if rule is None:\n",
    "                break\n",
    "\n",
    "            # If we get the rule with accuracy and coverage above threshold\n",
    "            \n",
    "            if rule.accuracy >= min_accuracy:\n",
    "                rules.append(rule)\n",
    "\n",
    "                # We're going to say that `subset` is the rows that need to be removed. \n",
    "                # We can drop by matching up the indices\n",
    "                current_data = current_data.drop(index=subset.index)\n",
    "                   \n",
    "            else:\n",
    "                done = True         \n",
    "                \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If [tear production rate=reduced] then none. Coverage:12, accuracy: 1.0\n",
      "If [age=old, tear production rate=normal, spectacles=nearsighted, astigmatism=no] then none. Coverage:1, accuracy: 1.0\n",
      "If [spectacles=farsighted, astigmatism=yes, age=medium] then none. Coverage:1, accuracy: 1.0\n",
      "If [age=old, spectacles=farsighted, astigmatism=yes] then none. Coverage:1, accuracy: 1.0\n",
      "If [astigmatism=no] then soft. Coverage:5, accuracy: 1.0\n",
      "If [astigmatism=yes] then hard. Coverage:4, accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "column_list = data.columns.to_numpy().tolist()\n",
    "rules = learn_rules_rf (column_list, data, None, 1, 0.95)\n",
    "for rule in rules[:20]:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "So we found a few differences, notably our dataset now rules covering all 24 points in the dataset, so we're perfectly identified here. The first rule is the same, but the second rule now goes through a bunch of different traits. We also have more complicated rules than we did before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 Marina Barsky. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
